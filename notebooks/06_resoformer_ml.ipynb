{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TinyAleph: ResoFormer - Machine Learning Architecture\n",
    "\n",
    "This notebook explores the ML components:\n",
    "\n",
    "- **SparsePrimeState**: Quaternionic amplitudes\n",
    "- **Tensor operations**: Pure Python implementation\n",
    "- **Layers**: Dense, Attention, Gating, Collapse\n",
    "- **ResoFormer**: Complete transformer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from tinyaleph.ml.sparse_state import (\n",
    "    SparsePrimeState, coherent_superposition, golden_superposition\n",
    ")\n",
    "from tinyaleph.ml.resoformer import (\n",
    "    Tensor, zeros, ones, randn, glorot_uniform,\n",
    "    Dense, QuaternionDense, LayerNorm, Dropout,\n",
    "    ResonantAttentionLayer, CoherenceGatingLayer, EntropyCollapseLayer,\n",
    "    ResoFormerBlock, ResoFormerConfig,\n",
    "    create_resoformer_model, create_resoformer_classifier\n",
    ")\n",
    "from tinyaleph.core.constants import PHI\n",
    "import math\n",
    "\n",
    "print(\"TinyAleph ML - ResoFormer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sparse Prime States\n",
    "\n",
    "States in H_Q = H_P ⊗ ℍ (Prime-Quaternion tensor product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sparse state\n",
    "state = SparsePrimeState.from_primes([2, 3, 5, 7])\n",
    "\n",
    "print(f\"SparsePrimeState with {len(state)} amplitudes\")\n",
    "print(f\"\\nQuaternion amplitudes:\")\n",
    "for prime, q in state:\n",
    "    print(f\"  |{prime}⟩: ({q.w:.3f}, {q.i:.3f}i, {q.j:.3f}j, {q.k:.3f}k)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Properties\n",
    "print(f\"Entropy: {state.entropy():.4f}\")\n",
    "print(f\"Is coherent: {state.is_coherent()}\")\n",
    "print(f\"\\nPrime spectrum:\")\n",
    "for p, prob in state.prime_spectrum().items():\n",
    "    print(f\"  P({p}) = {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Golden superposition - uses golden angle spacing\n",
    "golden = golden_superposition(5)\n",
    "\n",
    "print(\"Golden superposition (2π/φ² ≈ 137.5° spacing):\")\n",
    "for p, q in golden:\n",
    "    print(f\"  |{p}⟩: ({q.w:.3f}, {q.i:.3f}i)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors\n",
    "z = zeros((3, 4))\n",
    "o = ones((2, 3))\n",
    "r = randn((4, 4))\n",
    "g = glorot_uniform((32, 64))\n",
    "\n",
    "print(f\"zeros((3, 4)): shape = {z.shape}\")\n",
    "print(f\"ones((2, 3)): shape = {o.shape}\")\n",
    "print(f\"randn((4, 4)): shape = {r.shape}\")\n",
    "print(f\"glorot_uniform((32, 64)): shape = {g.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor operations\n",
    "a = Tensor([1.0, 2.0, 3.0, 4.0])\n",
    "b = Tensor([0.5, 0.5, 0.5, 0.5])\n",
    "\n",
    "print(f\"a = {a.data}\")\n",
    "print(f\"b = {b.data}\")\n",
    "print(f\"\\na + b = {(a + b).data}\")\n",
    "print(f\"a * b = {(a * b).data}\")\n",
    "print(f\"a.sum() = {a.sum().data}\")\n",
    "print(f\"a.mean() = {a.mean().data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "\n",
    "print(f\"x = {x.data}\")\n",
    "print(f\"relu(x) = {x.relu().data}\")\n",
    "print(f\"sigmoid(x) = {[round(v, 3) for v in x.sigmoid().data]}\")\n",
    "print(f\"tanh(x) = {[round(v, 3) for v in x.tanh().data]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard dense layer\n",
    "dense = Dense(units=16, activation=\"relu\", name=\"dense1\")\n",
    "\n",
    "x = randn((4, 8))  # batch=4, input_dim=8\n",
    "y = dense(x)\n",
    "\n",
    "print(f\"Dense layer:\")\n",
    "print(f\"  Input: {x.shape}\")\n",
    "print(f\"  Output: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quaternion dense layer\n",
    "q_dense = QuaternionDense(units=8, name=\"q_dense1\")\n",
    "\n",
    "x_q = randn((4, 32))\n",
    "y_q = q_dense(x_q)\n",
    "\n",
    "print(f\"QuaternionDense layer:\")\n",
    "print(f\"  Input: {x_q.shape}\")\n",
    "print(f\"  Output: {y_q.shape} (units × 4 quaternion components)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resonant attention\n",
    "attn = ResonantAttentionLayer(num_heads=4, key_dim=16, name=\"attn1\")\n",
    "\n",
    "x = randn((2, 8, 64))  # batch=2, seq=8, embed=64\n",
    "y = attn(x)\n",
    "\n",
    "print(f\"ResonantAttentionLayer (4 heads, key_dim=16):\")\n",
    "print(f\"  Input: {x.shape}\")\n",
    "print(f\"  Output: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Special Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coherence gating\n",
    "gating = CoherenceGatingLayer(threshold=1/PHI, name=\"gate1\")\n",
    "\n",
    "x = randn((4, 32))\n",
    "result = gating(x)\n",
    "\n",
    "print(f\"CoherenceGatingLayer (threshold=1/φ):\")\n",
    "print(f\"  Input: {x.shape}\")\n",
    "print(f\"  Output: {result['output'].shape}\")\n",
    "print(f\"  Coherence: {result['coherence'].shape}\")\n",
    "print(f\"  Gate: {result['gate'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy collapse (VQ-style)\n",
    "collapse = EntropyCollapseLayer(num_attractors=16, name=\"collapse1\")\n",
    "\n",
    "x = randn((4, 32))\n",
    "result = collapse(x, training=True)\n",
    "\n",
    "print(f\"EntropyCollapseLayer (16 attractors):\")\n",
    "print(f\"  Input: {x.shape}\")\n",
    "print(f\"  Output: {result['output'].shape}\")\n",
    "print(f\"  Probs: {result['probs'].shape}\")\n",
    "print(f\"  Entropy: {[round(e, 3) for e in result['entropy'].data]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ResoFormer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete transformer block\n",
    "block = ResoFormerBlock(\n",
    "    dim=64,\n",
    "    num_heads=4,\n",
    "    ffn_dim=256,\n",
    "    dropout_rate=0.1,\n",
    "    use_collapse=True,\n",
    "    name=\"block1\"\n",
    ")\n",
    "\n",
    "print(\"ResoFormerBlock architecture:\")\n",
    "print(\"  1. LayerNorm → Attention → Residual\")\n",
    "print(\"  2. ResonanceOperator\")\n",
    "print(\"  3. LayerNorm → FFN → Residual\")\n",
    "print(\"  4. CoherenceGating\")\n",
    "print(\"  5. EntropyCollapse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "x = randn((2, 8, 64))  # batch=2, seq=8, dim=64\n",
    "y = block(x, training=True)\n",
    "\n",
    "print(f\"\\nForward pass:\")\n",
    "print(f\"  Input: {x.shape}\")\n",
    "print(f\"  Output: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ResoFormer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = ResoFormerConfig(\n",
    "    vocab_size=1000,\n",
    "    seq_len=32,\n",
    "    dim=64,\n",
    "    num_layers=2,\n",
    "    num_heads=4,\n",
    "    ffn_dim=128,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "print(\"ResoFormerConfig:\")\n",
    "print(f\"  vocab_size: {config.vocab_size}\")\n",
    "print(f\"  seq_len: {config.seq_len}\")\n",
    "print(f\"  dim: {config.dim}\")\n",
    "print(f\"  num_layers: {config.num_layers}\")\n",
    "print(f\"  num_heads: {config.num_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language model\n",
    "lm = create_resoformer_model(\n",
    "    vocab_size=1000,\n",
    "    seq_len=32,\n",
    "    dim=64,\n",
    "    num_layers=2,\n",
    "    num_heads=4,\n",
    "    ffn_dim=128,\n",
    ")\n",
    "\n",
    "tokens = Tensor([list(range(8))], (1, 8))  # batch=1, seq=8\n",
    "logits = lm(tokens, training=False)\n",
    "\n",
    "print(f\"Language Model:\")\n",
    "print(f\"  Input tokens: {tokens.shape}\")\n",
    "print(f\"  Output logits: {logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier\n",
    "clf = create_resoformer_classifier(\n",
    "    vocab_size=1000,\n",
    "    seq_len=32,\n",
    "    dim=64,\n",
    "    num_layers=2,\n",
    "    num_heads=4,\n",
    "    ffn_dim=128,\n",
    "    num_classes=10,\n",
    ")\n",
    "\n",
    "output = clf(tokens, training=False)\n",
    "\n",
    "print(f\"Classifier:\")\n",
    "print(f\"  Input: {tokens.shape}\")\n",
    "print(f\"  Output: {output.shape} (10 classes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**SparsePrimeState:**\n",
    "- H_Q = H_P ⊗ ℍ (Prime-Quaternion tensor)\n",
    "- Quaternion amplitudes per prime\n",
    "- Entropy and coherence metrics\n",
    "\n",
    "**Layers:**\n",
    "- Dense and QuaternionDense\n",
    "- ResonantAttentionLayer (multi-head)\n",
    "- CoherenceGatingLayer (threshold-based)\n",
    "- EntropyCollapseLayer (VQ-style)\n",
    "\n",
    "**ResoFormer:**\n",
    "- Pre-norm transformer blocks\n",
    "- Resonance operators for phase rotation\n",
    "- Coherence-gated computation\n",
    "- Model variants: LM, Classifier, Embedder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}